{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train linear model on predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use a Dense() layer to convert the 1,000 predictions given by our model into a probability of dog vs cat\n",
    "- Train a linear model to take the 1,000 predictions as input, and return dog or cat as output, learning from the Kaggle data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config steps\n",
    "- Copy a small amount of our data into a 'sample' directory, with the exact same structure as our 'train' directory\n",
    "- Should do all of initial testing using a dataset small enough that we don't have to wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"data/redux/sample/\"\n",
    "#path = \"data/redux/\"\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial and error to find the max batch size\n",
    "- The max size doesn't give an 'out of memory' error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batch_size = 100\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with VGG16 model because we'll use its predictions and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our overall approach:\n",
    "1. Get the true labels for every image: isCat or isDog.\n",
    "2. Get the 1,000 ImageNet category predictions for every image.\n",
    "3. Feed these predictions as input to a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab training and validation batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Use batch size of 1 since we're just doing preprocessing on the CPU\n",
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=1)\n",
    "batches = get_batches(path+'train', shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid loading the images every time we want to use them, we should save the processed arrays.\n",
    "- The fastest way to save and load numpy arrays, use bcolz. This also compresses the arrays, so we save disk space. \n",
    "- The provided functions (save_array, load_array) are defined in utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'get_data' function joins the arrays from all the batches.\n",
    "Use this to grab the training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data = get_data(path+'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "trn_data = get_data(path+'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 3, 224, 224)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Get true labels for every image\n",
    "Keras returns classes as a single column, so we convert them to one hot encoding - using onehot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_classes[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_labels[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each image, features for our linear model include\n",
    "- the label\n",
    "- 1,000 imagenet probabilties from VGG16\n",
    "\n",
    "#### (2) Get the 1,000 ImageNet category predictions for every image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate predictions for the input samples.\n",
    "trn_features = model.predict(trn_data, batch_size=batch_size)\n",
    "val_features = model.predict(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path + 'train_lastlater_features.bc', trn_features)\n",
    "save_array(model_path + 'valid_lastlater_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Feed these predictions as input to a linear model.\n",
    "Define our linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1000 inputs (the saved features), and 2 outputs (for dog & cat)\n",
    "lm = Sequential([ Dense(2, activation='softmax', input_shape=(1000,)) ])\n",
    "lm.compile(optimizer=RMSprop(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batch_size=64\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16 samples, validate on 8 samples\n",
      "Epoch 1/3\n",
      "16/16 [==============================] - 0s - loss: 0.5888 - acc: 0.6250 - val_loss: 0.4288 - val_acc: 0.8750\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/3\n",
      "16/16 [==============================] - 0s - loss: 0.3033 - acc: 1.0000 - val_loss: 0.3214 - val_acc: 0.8750\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/3\n",
      "16/16 [==============================] - 0s - loss: 0.2207 - acc: 1.0000 - val_loss: 0.2494 - val_acc: 0.8750\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4c93646490>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(trn_features, trn_labels, nb_epoch=3, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_4 (Dense)                  (None, 2)             2002        dense_input_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 2002\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain last layer (linear model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG16 network's last layer is Dense (i.e. a linear model).\n",
    "- It seems a little odd that we are adding an additional linear model on top of it.\n",
    "\n",
    "The last layer had a softmax activation.\n",
    "- which is an odd choice for an intermediate layer because we're going to add an extra layer on top of it.\n",
    "\n",
    "What if we removed the last layer and replaced it with one trained for classifying cats and dogs?\n",
    "\n",
    "Steps:\n",
    "- Remove the last layer.\n",
    "- Tell Keras to fix the weights in all the other layers (since we aren't looking to learn new parameters for those other layers).\n",
    "- Add a new layer trained for classifying cats and dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 3, 224, 224)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 3, 226, 226)   0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 224, 224)  1792        zeropadding2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 64, 226, 226)  0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 224, 224)  36928       zeropadding2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_3 (ZeroPadding2D)  (None, 64, 114, 114)  0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 112, 112) 73856       zeropadding2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_4 (ZeroPadding2D)  (None, 128, 114, 114) 0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 112, 112) 147584      zeropadding2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_5 (ZeroPadding2D)  (None, 128, 58, 58)   0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 256, 56, 56)   295168      zeropadding2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_6 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_7 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 256, 30, 30)   0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 512, 28, 28)   1180160     zeropadding2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_9 (ZeroPadding2D)  (None, 512, 30, 30)   0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 512, 28, 28)   2359808     zeropadding2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_10 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_11 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_12 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_13 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 512, 7, 7)     0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 25088)         0           maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 4096)          102764544   flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4096)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 4096)          16781312    dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 4096)          0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1000)          4097000     dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 138357544\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#look at the layers of our VGG model\n",
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove the last layer\n",
    "model.pop()\n",
    "#Tell Keras to fix the weights in all the other layers \n",
    "for layer in model.layers: layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Careful!** Be careful not to rerun any code in the previous sections, without first recreating the model from scratch!\n",
    "\n",
    "Add our new final layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#View source code for finetune function\n",
    "??vgg.finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove the last layer & add a new layer are done in ft function\n",
    "??vgg.ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile our updated model and set up our batches to use the preprocessed images \n",
    "- Note: we will shuffle the training batches, to add more randomness when using multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "#Takes numpy data & label arrays, and generates batches of augmented/normalized data. \n",
    "batches = gen.flow(trn_data, trn_labels, batch_size=batch_size, shuffle=True)\n",
    "val_batches = gen.flow(val_data, val_labels, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple function for fitting models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, batches, val_batches, nb_epoch=1):\n",
    "    model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=nb_epoch,\n",
    "                       validation_data=val_batches, nb_val_samples=val_batches.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it to train the last layer of our model.\n",
    "\n",
    "- It runs quite slowly, since it has to calculate all the previous layers in order to know what input to pass to the new final layer.\n",
    "- We could pre-calculate the output of the penultimate layer.\n",
    "- But since we're only likely to want one ore two iterations, it's easier to follow this alternative approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = RMSprop(lr=0.1)\n",
    "#Configure the model for training.\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "16/16 [==============================] - 254s - loss: 1.2539 - acc: 0.8125 - val_loss: 4.0295 - val_acc: 0.7500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/2\n",
      "16/16 [==============================] - 256s - loss: 7.2903 - acc: 0.4375 - val_loss: 4.0295 - val_acc: 0.7500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, batches, val_batches, nb_epoch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weights of all your models, so you can re-use them later. \n",
    "\n",
    "Be sure to note the git log number of your model when keeping a research journal of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_path+'finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 85s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.0295238494873047, 0.75]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Return the loss value & metrics values for the model in test mode.\n",
    "model.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the earlier prediction examples visualizations by redefining probs and preds, and re-using our earlier code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 85s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "8/8 [==============================] - 85s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  1.0000e+00,   1.0000e+00,   1.0000e+00,   1.0000e+00,   9.0465e-38,   1.0000e+00,\n",
       "         0.0000e+00,   0.0000e+00], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate class predictions for the input samples, batch by batch.\n",
    "preds = model.predict_classes(val_data, batch_size=batch_size)\n",
    "#Generate class probability predictions for the input samples, batch by batch.\n",
    "probs = model.predict_proba(val_data, batch_size=batch_size)[:,0]\n",
    "probs[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(val_classes, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 1]\n",
      " [1 2]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAEmCAYAAADr3bIaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmY5OFmameXw/wE7qxFWDP8FIwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f478eefc510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm, {'cat':0, 'dog':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??model.predict_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??model.predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
